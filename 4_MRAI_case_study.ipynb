{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Heart disease is the leading cause of death globally. __Heart failure__ is the primary cause of heart diseases, and that refers to a physiological state in which the heart is unable to pump sufficiently to maintain blood flow to meet the body's need. It usually occurs because the heart has become too weak or stiff. \n",
    "\n",
    "HF is routinely diagnosed by:\n",
    "* __Electrocardiogram (ECG)__, which measure heart's rhythm and electrical activity. From the ECG the QRS duration can be computed. An increased QRS duration, indicate dyssynchronous contraction and relaxation of the left and right ventricles and it is a marker of detection of HF subjects.\n",
    "* __MRI__ or __US imaging__ which provides structural and functional information of the heart. Evaluation of the structure and the function of the ventricles can provide useful information for diagnosis and characterization of disease. \n",
    " \t \t\n",
    "Based on the result of these tests, doctors use NYHA class to classify patients' heart failure according to the severity of their symptoms:\n",
    "\n",
    "__Class I__: no limitation is experienced in any activities; there are no symptoms from ordinary activities.\n",
    "\n",
    "__Class II__: slight, mild limitation of activity; the patient is comfortable at rest or with mild exertion.\n",
    "\n",
    "__Class III__: marked limitation of any activity; the patient is comfortable only at rest.\n",
    "\n",
    "__Class IV__: any physical activity brings on discomfort and symptoms occur at rest.\n",
    "\n",
    "### Current diagnostic technique\n",
    "\n",
    "Currently, the parameters used in clinics to identify cardiac patients are:\n",
    "* __LVEDV__ which is the maximum amount of blood that heart can pump\n",
    "* __LVESV__ which is the minium amount of blood that heart can pump\n",
    "* __LVSV__ which is the amount of blood pumped by the left ventricle of the heart in one contraction. \n",
    "* __Ejection fraction (LVEF)__, which computes the amount of blood of the left ventricle (LV) pumps out with each contraction. A normal heartâ€™s ejection fraction may be between 50 and 70 percent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The objective of this workshop is twofold:\n",
    "\n",
    "1. Improve the quality of a pre-trained segmentation neural network to segment the left ventricle (LV) of the heart\n",
    "2. Train a simple deep neural network (DNN) to classify between healthy and heart failure subjects using clinical metrics, i.e. LVEDV, LVESV, LVSV and LVEF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Image segmentaion\n",
    "\n",
    "\n",
    "In this exercise we will segment the left ventricle of the heart in relatively small images using neural networks. Below is the code for setting up a segmentation network and training it. The network isn't very good, so the exercise is to improve the quality of the segmentation by improving the network. \n",
    "\n",
    "The data being used here is derived from the Sunnybrook Dataset (https://www.cardiacatlas.org/studies/sunnybrook-cardiac-data/) of cardiac images, filtered to contain only left ventricle myocardium segmentations and reduced in XY dimensions.\n",
    "\n",
    "PyTorch documentation can be found here: https://pytorch.org/docs/stable/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import torch\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![ -f scd_lvsegs.npz ] || wget https://github.com/estherpuyol/MRAI_workshop/raw/master/scd_lvsegs.npz\n",
    "\n",
    "data=np.load('scd_lvsegs.npz') # load all the data from the archive\n",
    "\n",
    "images=data['images'] # images in BHW array order\n",
    "segs=data['segs'] # segmentations in BHW array order\n",
    "caseIndices=data['caseIndices'] # the indices in `images` for each case\n",
    "\n",
    "images=images.astype(np.float32)/images.max() # normalize images\n",
    "\n",
    "plt.imshow(images[13]+segs[13]*0.25,cmap='gray') # show image 13 with segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test\n",
    "\n",
    "For this exercise, we will divide the data between train and test data, choosing to retain the last 6 cases as our test set. Typically you would want to have a training dataset, a validation dataset checked periodically during training to ensure the network continues to produce good results for data is isn't trained on, and a test dataset used to validate the training after the fact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testIndex = 6  # keep the last 6 cases for testing\n",
    "\n",
    "# divide the images, segmentations, and categories into train/test sets\n",
    "trainImages, trainSegs = images[:-testIndex], segs[:-testIndex]\n",
    "testImages, testSegs = images[-testIndex:], segs[-testIndex:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define segmentation network\n",
    "\n",
    "As loss function, for this exercise we will use the binary Dice loss, which measure the overlap between the ground truth segmentation and the predicted segmentation\n",
    "\n",
    "As neural network, we used a simple auto-encoder, which which has a down-sampling path with three layers using strided convolutions to reduce dimensions. After the bottom layer the decoding path upsamples using strided transpose convolutions. Dropout is added in places to reduce over-fitting.\n",
    "\n",
    "However, the current network architecture is doing poorly. Modify the current network to improve the segmentation accuracy. Some of the possible solutions are:\n",
    "\n",
    "- Change kernel size\n",
    "- Add layers\n",
    "- Change dropout value\n",
    "- Change activation function\n",
    "- Think of another strategy for batch normalisation\n",
    "- Add data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice loss \n",
    "class DiceLoss(nn.modules.loss._Loss):\n",
    "    '''This defines the binary dice loss function used to assess segmentation overlap.'''\n",
    "    def forward(self, source, target, smooth=1e-5):\n",
    "        batchsize = target.size(0)\n",
    "        source=source.sigmoid() # apply sigmoid to the source logits to impose it onto the [0,1] interval\n",
    "        \n",
    "        # flatten target and source arrays to 2D BV arrays\n",
    "        tsum=target.view(batchsize, -1) \n",
    "        psum=source.view(batchsize, -1)\n",
    "        \n",
    "        intersection=psum*tsum\n",
    "        sums=psum+tsum \n",
    "\n",
    "        # compute the score, the `smooth` value is used to smooth results and prevent divide-by-zero\n",
    "        score = 2.0 * (intersection.sum(1) + smooth) / (sums.sum(1) + smooth)\n",
    "        \n",
    "        # `score` is 1 for perfectly identical source and target, 0 for entirely disjoint\n",
    "        return 1 - score.sum() / batchsize\n",
    "\n",
    "# Segmentation network\n",
    "class SegNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model=nn.Sequential(\n",
    "            # layer 1: convolution, normalization, downsampling\n",
    "            nn.Conv2d(1,2,3,1,1),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3,2,1),\n",
    "            # layer 2\n",
    "            nn.Conv2d(2,4,3,1,1),\n",
    "            # layer 3\n",
    "            nn.ConvTranspose2d(4,2,3,2,1,1),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "            # layer 4: output\n",
    "            nn.Conv2d(2,1,3,1,1),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network\n",
    "\n",
    "Once we have defined the network, we need to train it. An strategy is to load the whole dataset into memory at once, but typically this isn't possible due to memory restrictions. A different strategy is to create batches for each train step, which means that in each iteration only few image (batch) would be used for training. This strategy is handled by the data loader.\n",
    "\n",
    "It is also possible to apply some transformations to the data before being forward passed through the network, this is generally called data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# store the training data as tensors\n",
    "trainTensor=torch.from_numpy(trainImages[:,None])\n",
    "segTensor=torch.from_numpy(trainSegs[:,None].astype(np.float32))\n",
    "\n",
    "# create network object\n",
    "net=SegNet()\n",
    "\n",
    "# choose a device and move the net and tensors to its memory\n",
    "device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net=net.to(device)\n",
    "trainTensor=trainTensor.to(device)\n",
    "segTensor=segTensor.to(device)\n",
    "\n",
    "# define optimizer and loss function\n",
    "opt=torch.optim.Adam(net.parameters(),0.005)\n",
    "loss=DiceLoss()\n",
    "\n",
    "trainSteps=5000 # changed from 100\n",
    "losses=[]\n",
    "\n",
    "# run through training steps\n",
    "for t in range(1,trainSteps+1):\n",
    "    opt.zero_grad()\n",
    "    pred=net(trainTensor)\n",
    "    lossval=loss(pred,segTensor)\n",
    "    lossval.backward()\n",
    "    opt.step()\n",
    "        \n",
    "    losses.append(lossval.item())\n",
    "    if t%(trainSteps//20)==0:\n",
    "        print(t,lossval.item())    \n",
    "    \n",
    "\n",
    "# sample an image from the training data and look at the segmentation the network predicted for it\n",
    "sample=10\n",
    "predSample=pred[sample,0].cpu().data.numpy()\n",
    "fig,ax=plt.subplots(1,5,figsize=(20,5))\n",
    "ax[0].set_title('Loss')\n",
    "ax[0].semilogy(losses)\n",
    "ax[1].set_title('Sample Image')\n",
    "ax[1].imshow(trainImages[10])\n",
    "ax[2].set_title('Sample Ground Truth')\n",
    "ax[2].imshow(trainSegs[10])\n",
    "ax[3].set_title('Sample Logits')\n",
    "ax[3].imshow(predSample)\n",
    "ax[4].set_title('Sample Predicted Segmentation')\n",
    "ax[4].imshow(predSample>0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "Once the network is been train, we can now apply the test data to the network. These images were never seen by the network so how well the task is performed is an indicator of how generalized and robust the network is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testTensor=torch.from_numpy(testImages[:,None]).to(device)\n",
    "testSegTensor=torch.from_numpy(testSegs[:,None]).to(device).float()\n",
    "\n",
    "net.to(device)\n",
    "net.eval()\n",
    "pred=net(testTensor)\n",
    "testloss=loss(pred,testSegTensor).item() # calculate test loss\n",
    "\n",
    "pred=pred.to('cpu').data.numpy()\n",
    "\n",
    "seg=pred>0.5\n",
    "\n",
    "fig,ax=plt.subplots(1,4,figsize=(20,5))\n",
    "ax[0].imshow(testImages[10])\n",
    "ax[1].imshow(testSegs[10])\n",
    "ax[2].imshow(pred[10,0])\n",
    "ax[3].imshow(seg[10,0])\n",
    "\n",
    "print('Test loss:',testloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification using clinical metrics \n",
    "\n",
    "In this part of the workshop, we aim to use the segmentations produced from the previous segmentation network to compute the following clinical metrics LVEDV, LVESV, LVSV and LVEF to classify the between healthy and heart failure subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN network\n",
    "\n",
    "For classification, we will use a deep neural network classifier (DNN) with two layers with sizes 20 and 10. To evaluate the performances of the classifier we will compute accuracy, precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, yPred, target_names):\n",
    "    \"\"\" Compute metrics\n",
    "    Parameters\n",
    "    ----------\n",
    "    yPred: labels predicted using DNN\n",
    "    y_true: ground truth labels\n",
    "    target_names:  names matching the labels\n",
    "    \"\"\"\n",
    "\n",
    "    BACC = balanced_accuracy_score(y_true, yPred)\n",
    "    PRE = precision_score(y_true, yPred, average=None)\n",
    "    REC = recall_score(y_true, yPred, average=None)\n",
    "    print('Balanced accuracy  {0:.2f}'.format(BACC))\n",
    "\n",
    "    headers = [\"precision\", \"recall\"]\n",
    "\n",
    "    rows = zip(target_names, PRE, REC)\n",
    "\n",
    "    digits = 2\n",
    "    longest_last_line_heading = 'weighted avg'\n",
    "    name_width = max(len(cn) for cn in target_names)\n",
    "    width = max(name_width, len(longest_last_line_heading), digits)\n",
    "    head_fmt = u'{:>{width}s} ' + u' {:>9}' * len(headers)\n",
    "    report = head_fmt.format(u'', *headers, width=width)\n",
    "    report += u'\\n\\n'\n",
    "    row_fmt = u'{:>{width}s} ' + u' {:>9.{digits}f}' + u' {:>9}\\n'\n",
    "    for row in rows:\n",
    "        report += row_fmt.format(*row, width=width, digits=digits)\n",
    "    report += u'\\n'\n",
    "\n",
    "    print(report)\n",
    "\n",
    "    return BACC * 100, PRE * 100, REC * 100\n",
    "  \n",
    "# Create a DNN network for classification with two layers with sizes 20 and 10. \n",
    "class DNN(torch.nn.Module):\n",
    "    '''Plain dense neural network of linear layers using dropout and ReLU activation.\n",
    "    Size of the hidden layers: [20,10]\n",
    "    numClasses: number of classes used for the classification'''\n",
    "    def __init__(self, numClasses):\n",
    "        super().__init__()\n",
    "        dropout_value = 0.05\n",
    "        ################\n",
    "        # TO COMPLETE\n",
    "        ################\n",
    "    def forward(self, x):\n",
    "        ################\n",
    "        # TO COMPLETE\n",
    "        ################\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "To ensure that everyone work with the same data, download the following file that contains the images segmented with the previous network optimised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![ -f data.npz ] || wget https://github.com/estherpuyol/MRAI_workshop/raw/master/data.npz\n",
    "data_file_name = 'data.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute clinical metricas and save them in a matrix called metrics\n",
    "- LVEDV (maximum volume of the cardiac cycle)\n",
    "- LVESV (minimum volume of the cardiac cycle)\n",
    "- LVSV = LVEDV - LVESV \n",
    "- LVEF = LVSV/LVEDV*100 \n",
    "\n",
    "Then:\n",
    "- Generate a matrix ```metrics``` that concatenate LVEDV, LVESV, LVSV and LVEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.load(data_file_name)\n",
    "\n",
    "# Save fields inside data\n",
    "images = data['images']  # (805, 128, 128)\n",
    "segs = data['segs']  # (805, 128, 128)\n",
    "caseNames = data['caseNames']  # 45\n",
    "caseIndices = data['caseIndices']  # 45\n",
    "caseTypeNames = ['Normal', 'Heart Failure']\n",
    "caseTypes = data['caseTypes']  # 45\n",
    "caseTypes[caseTypes == 2] = 0\n",
    "caseVoxelSize = data['caseVoxelSize']\n",
    "isEDImg = data['isEDImg']  # 805\n",
    "segTypes = data['segTypes']  # ['Background', 'LV Pool']\n",
    "\n",
    "\n",
    "# Compute LVEDV, LVESV, LVSV and LVEF\n",
    "metrics = np.zeros((len(caseNames), 4))\n",
    "for ii, ind in enumerate(caseIndices):\n",
    "    indED_pat = isEDImg[ind[0]:ind[1]]\n",
    "    img_pat = images[ind[0]:ind[1], :, :]\n",
    "    seg_pat = segs[ind[0]:ind[1], :, :]\n",
    "    img_ED = img_pat[indED_pat, :, :]\n",
    "    img_ES = img_pat[~indED_pat, :, :]\n",
    "    seg_ED = seg_pat[indED_pat, :, :]\n",
    "    seg_ES = seg_pat[~indED_pat, :, :]\n",
    "    dx, dy, dz = caseVoxelSize[ii]\n",
    "    volume_per_voxel = dx * dy * dz * 1e-3\n",
    "    density = 1.05\n",
    "\n",
    "    metrics[ii, 0] = np.sum(np.sum(seg_ED == 1, axis=1)) * volume_per_voxel\n",
    "    metrics[ii, 1] = np.sum(np.sum(seg_ES == 1, axis=1)) * volume_per_voxel\n",
    "    metrics[ii, 2] = metrics[ii, 0] - metrics[ii, 1]\n",
    "    metrics[ii, 3] = metrics[ii, 2] / metrics[ii, 0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data\n",
    "\n",
    "Plot LVEDV, LVESV, LVSV and LVEF per groups, i.e. LVEDV and LVESV, LVSV and LVEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotTwoClassData(X, y, title, _caseTypeNames):\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], c='r', label=_caseTypeNames[0])\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='b', label=_caseTypeNames[1])\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    \n",
    "\n",
    "plt.figure(figsize = [14,5])\n",
    "plt.subplot(121)\n",
    "PlotTwoClassData(metrics[:,0:2], caseTypes, 'Healthy vs Heart Failure', caseTypeNames)\n",
    "plt.xlabel('LVEDV')\n",
    "plt.ylabel('LVESV')\n",
    "plt.subplot(122)\n",
    "PlotTwoClassData(metrics[:,2:], caseTypes, 'Healthy vs Heart Failure', caseTypeNames)\n",
    "plt.xlabel('LVSV')\n",
    "plt.ylabel('LVEF')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test\n",
    "\n",
    "Data is unbalanced (there is no the same number of patients per group). We need to ensure that in the split training/test there is balanced number of classed. To this end use StratifiedShuffleSplit from scikit-learn that will automatically ensure that.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "for train_index, test_index in sss.split(metrics, caseTypes):\n",
    "    X_train, X_test = metrics[train_index, :], metrics[test_index, :]\n",
    "    y_train, y_test = caseTypes[train_index], caseTypes[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a DNN network to perform classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# TO COMPLETE\n",
    "################\n",
    "\n",
    "#1. Instance the DNN network\n",
    "nb_iters = ...\n",
    "output_size = ...\n",
    "net = DNN(output_size)\n",
    "\n",
    "xt = torch.from_numpy(...).type(torch.FloatTensor)\n",
    "yt = torch.from_numpy(...).type(torch.LongTensor)\n",
    "\n",
    "# Select the loss fuction for classification\n",
    "# Docs: https://pytorch.org/docs/stable/nn.html\n",
    "loss_func = ... \n",
    "\n",
    "# Select the Optimizer with lr = 0.0001\n",
    "# Docs: https://pytorch.org/docs/stable/optim.html\n",
    "opt = ...\n",
    "\n",
    "losses = []\n",
    "\n",
    "for i in range(1, nb_iters + 1):\n",
    "\n",
    "    out = net(xt)  # input x and predict based on x\n",
    "    loss = loss_func(out, yt)   \n",
    "\n",
    "    # clear gradients for next train\n",
    "    ...\n",
    "    # backpropagation, compute gradients\n",
    "    ...\n",
    "    # apply gradients\n",
    "    ...\n",
    "    # Stor the loss over iteration \n",
    "    ...\n",
    "\n",
    "    if i % (nb_iters // 10) == 0:\n",
    "        print(i, losses[-1])\n",
    "\n",
    "# Plot total loss over iterations\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy network and compute metrics\n",
    "\n",
    "First deploy the trained netowrk and then compute the following metrics: balanced accuracy, precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# TO COMPLETE\n",
    "################\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    out = torch.from_numpy(...).float()\n",
    "    # Deploy DNN for the test dataset and compute the predicted label\n",
    "    out = ... \n",
    "    # Convert to numpy array\n",
    "    out = ... \n",
    "    # Select the highest probability for each input \n",
    "    y_pred = np.argmax(out, axis = 1)\n",
    "    # Compute metrics\n",
    "    ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
